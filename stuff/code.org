* Topics
- Feature extraction
- TARTYP: 2D analysis or one-for-all
- Visualization
- Testing dataset

  
* Sort files and copy (2d)

This script relies on the dataset SCHAEFFER that should be extracted at ../../dataset/SCHAEFFER/ relative to this script. The dataset may be downloaded from https://www.kaggle.com/datasets/maurizioberta/test-schaeffer?resource=download

This script parses through the SCHAEFFER dataset and looks for the keywords in the variables keyA and keyB. It goes through the following steps:
1. clear the directories defined by dirA and dirB
2. loop through the directories in the SCHEAFFER set
3. loops through the json in each directory and parses it for keyA and keyB
4. copies over the found files to the respective directory in /Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/classification/training_data
5. closes open files

To find additional classifiers its easy to run the script with different key words. Beware of multi word keys which has not been tested.

#+begin_src shell :results output :tangle ./import_data.sh
  #!/bin/bash
  dataset="../../dataset/SCHAEFFER/"
  keyA=Impulse
  keyB=Iteration
  base="../classification/training_data/"
  dirA="$base""$keyA"
  dirB="$base""$keyB"

  echo "Clearing directories...\n\n"
  rm -rf $dirB/*.wav
  rm -rf $dirA/*.wav

  echo "Copying files from dataset...\n\n"
  for d in "$dataset"*/; do
      for j in "$d"*.json; do
  	if test -f "$j"
  	then
  	    if [[ "$(cat "$j" | jq '.object.labels.sustain')" == "\""$keyB"\"" ]]; then
  		if [ ! -d "$dirB" ]; then
  		    mkdir "$dirB"
  		fi
  		cp "$d""`cat "$j" | jq -r '.object.filename'`" "$dirB"
  	    fi
  	    if [[ "$(cat "$j" | jq '.object.labels."pulse-typology"')" == "\""$keyA"\"" ]]; then
  		if [ ! -d "$dirA" ]; then
  		    mkdir "$dirA"
  		fi
  		cp "$d""`cat "$j" | jq -r '.object.filename'`" "$dirA"
  	    fi
  	fi
      done
  done

  echo "Done!\n"
#+end_src

#+RESULTS:
: Clearing directories...\n\n
: Copying files from dataset...\n\n
: Done!\n

* Sort files and copy (add dimension)
This script relies on the dataset SCHAEFFER that should be extracted at ../../dataset/SCHAEFFER/ relative to this script. The dataset may be downloaded from https://www.kaggle.com/datasets/maurizioberta/test-schaeffer?resource=download

See instruction [[*Sort files and copy (2d)][above]].

To find additional classifiers its easy to run the script with different key words. Use the parameter 'column' to specify in which column the keyword is found in.

#+begin_src shell :results output :tangle ./import_data.sh
  #!/bin/bash
  dataset="../../dataset/SCHAEFFER/"
  keyA="Vacillating sustain"
  keyB="Flat sustain"
  base="../classification/training_data/"
  dirA="$base""$keyA"
  dirB="$base""$keyB"
  path=".object.labels.sustain"

  echo "Clearing directories...\n\n"
  if [ ! -d "$dirB" ]; then
     rm -rf $dirB/*.wav
  fi
  if [ ! -d "$dirA" ]; then
      rm -rf $dirA/*.wav
  fi

  echo "Copying files from dataset...\n\n"
  for d in "$dataset"*/; do
      for j in "$d"*.json; do
  	if test -f "$j"
  	then
  	    if [[ "$(cat "$j" | jq $path)" == "\""$keyA"\"" ]]; then
  		if [ ! -d "$dirA" ]; then
  		    mkdir "$dirA"
  		fi
  		cp "$d""`cat "$j" | jq -r '.object.filename'`" "$dirA"
  	    fi
  	    if [[ "$(cat "$j" | jq $path)" == "\""$keyB"\"" ]]; then
  		if [ ! -d "$dirB" ]; then
  		    mkdir "$dirB"
  		fi
  		cp "$d""`cat "$j" | jq -r '.object.filename'`" "$dirB"
  	    fi
  	fi
      done
  done

  echo "Done!\n"
#+end_src

* Sort files and copy full TARTYP

** Map
All children of .object.labels:
|                                              | formed                 | impulse         | formed            |
|                                              | held sounds            |                 | iterative sounds  |
|----------------------------------------------+------------------------+-----------------+-------------------|
| type in SCHAEFFER                            | sustain.'Flat sustain' | sustain.Impulse | sustain.Iteration |
|----------------------------------------------+------------------------+-----------------+-------------------|
| .'mass-type'.'Harmonic sound'                | HarmSus                | HarmImp         | HarmIter          |
|----------------------------------------------+------------------------+-----------------+-------------------|
| .'mass-type'.'Noisy sound'                   | NoiseSus               | NoiseImp        | NoiseIter         |
|----------------------------------------------+------------------------+-----------------+-------------------|
| .'mass-type'.'Vacillating sustain'           | VacillatingSus         |                 |                   |
| .'mass-type'.'Composite or Stratified sound' | -                      | CompositeImp    | CompositeIter     |
|----------------------------------------------+------------------------+-----------------+-------------------|
** Script

This script relies on the dataset SCHAEFFER that should be extracted at ../../dataset/SCHAEFFER/ relative to this script. The dataset may be downloaded from https://www.kaggle.com/datasets/maurizioberta/test-schaeffer?resource=download

This script parses through the SCHAEFFER dataset and looks for the keywords in the variables keyA and keyB. It goes through the following steps:
1. clear the directories defined by dirA and dirB
2. loop through the directories in the SCHEAFFER set
3. loops through the json in each directory and parses it for keyA and keyB
4. copies over the found files to the respective directory in /Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/classification/training_data
5. closes open files

To find additional classifiers its easy to run the script with different key words. Beware of multi word keys which has not been tested.

#+begin_src shell :results output :tangle ./import_data.sh
  #!/bin/bash
  dataset="../../dataset/SCHAEFFER/"

  harmsus=("HarmSus" "Harmonic Sound" "Flat sustain")
  harmimp=("HarmImp" "Harmonic Sound" "Impulse")
  harmiter=HarmIter
  noisesus=NoiseSus
  noiseimp=NoiseImp
  noiseiter=NoiseIter
  vacillatingsus=VacillatingSus
  compositeimp=CompositeImp
  compositeiter=CompositeIter 
  base="../classification/training_data"
  keyA=0
  keyB="$base""$harmimp"
  keyC="$base""$harmiter"
  keyD="$base""$noisesus"
  keyE="$base""$noiseimp"
  keyF="$base""$noiseiter"
  keyG="$base""$vacillatingsus"
  keyH="$base""$compositeimp"
  keyI="$base""$compositeiter"
  pathA='.object.labels."Flat sustain"'
  pathB='.object.labels.Impulse'
  pathC='.object.labels.Iteration'

  echo $base/${harmsus[0]}
  # echo "Clearing directories...\n"
  # for directory in keyA keyB keyC keyD keyE keyF keyG keyH keyI
  # do
  #     rm -rf $d/*.wav
  # done

  echo "Copying files from dataset...\n\n"
  for d in "$dataset"*/; do
      for j in "$d"*.json; do
      	if test -f "$j"
      	then
      	    if  [[ "$(cat "$j" | jq -r '.object.labels."mass-type"')" == "Harmonic sound" ]] &&  [[ "$(cat "$j" | jq -r '.object.labels.sustain')" == "Flat sustain" ]]; then
      		if [ ! -d "$dirB" ]; then
      		    mkdir "$dirB"
      		fi
  		 echo "$d""`cat "$j" | jq -r '.object.filename'`"
      		# cp "$d""`cat "$j" | jq -r '.object.filename'`" "$dirB"
      	    fi
      	fi
      done
  done

  echo "Done!\n"
#+end_src

#+RESULTS:
#+begin_example
../classification/training_data/HarmSus
Copying files from dataset...\n\n
../../dataset/SCHAEFFER/Antonietta Turano/canto armonicorisonanza.wav
../../dataset/SCHAEFFER/Bruno Emanuele Reggiani/Sound object - Drone (ML).wav
../../dataset/SCHAEFFER/Dario Gatto/Sax multiphonic 1_1.wav
../../dataset/SCHAEFFER/Gabriele Canova/16_TEXTURE ARMONICA.wav
../../dataset/SCHAEFFER/Giorgio Labagnara/Suono16.wav
../../dataset/SCHAEFFER/Giorgio Labagnara/Suono4.wav
../../dataset/SCHAEFFER/Michele Caldera/Bass Minimoog Slide Down.wav
../../dataset/SCHAEFFER/Padrini Federico/12.wav
../../dataset/SCHAEFFER/Padrini Federico/7.wav
../../dataset/SCHAEFFER/SALVATORE CADINU/031.wav
../../dataset/SCHAEFFER/Tommaso Cherchi/Impact_1.wav
Done!\n
#+end_example

* SCHAEFFER database labels
#+begin_src json
  {
    "Type":{
        "0":"Soundscape",
        "1":"Drone",
        "2":"Chop",
        "3":"Sub",
        "4":"Glitch",
        "5":"Impact",
        "6":"Stab (attacco risonanza)",
        "7":"Synthesis",
        "8":"Vocal",
        "9":"Scratch",
        "10":"Crackle",
        "11":"Noise",
        "12":"Textural",
        "13":"Instrumental",
        "14":"Chirp",
        "15":"Percussive",
        "16":"Honk",
        "17":"Choral"
    },
    "Mass type":{
        "0":"Sinusoidal sound",
        "1":"Harmonic sound",
        "2":"Inharmonic sound",
        "3":"Cluster sound",
        "4":"Breathlike sound",
        "5":"Noisy sound",
        "6":"Composite or Stratified sound"
    },
    "Complexity":{
        "0":"Very simple element",
        "1":"Relatively simple element",
        "2":"Moderately complex element",
        "3":"Relatively complex element",
        "4":"Very complex element",
        "5":"Simple emergence from complex details"
    },
    "Onset":{
        "0":"Sharp onset",
        "1":"Marked onset",
        "2":"Flat onset",
        "3":"Swelled onset",
        "4":"Fade in"
    },
    "Sustain":{
        "0":"Flat sustain",
        "1":"Vacillating sustain",
        "2":"Ostinato ",
        "3":"Decaying sustain",
        "4":"Uplifting sustain",
        "5":"Iteration",
        "6":"Accumulation"
    },
    "Offset":{
        "0":"Sharp ending",
        "1":"Marked ending",
        "2":"Flat ending",
        "3":"Soft ending",
        "4":"Laissez vibrer",
        "5":"Interrupted resonance",
        "6":"Fade out"
    },
    "Pulse typology":{
        "0":"Impulse",
        "1":"Regular pulse train",
        "2":"Irregular pulse train",
        "3":"No pulse"
    },
    "Processes":{
        "0":"Chorus",
        "1":"Tremolo",
        "2":"Distortion",
        "3":"Fuzzy",
        "4":"Granular",
        "5":"Loop",
        "6":"Bit reduction",
        "7":"Reverb",
        "8":"Filtered",
        "9":"Resonators",
        "10":"Flanger",
        "11":"Pitch-shift",
        "12":"Stretched",
        "13":"Delay",
        "14":"Eco",
        "15":"Vibrato",
        "16":"Filter Modulation",
        "17":"Glissando"
    },
    "Directionality":{
        "0":"Forward push",
        "1":"Evaded forward push",
        "2":"Suspended forward push",
        "3":"Backward push",
        "4":"Neutral"
    }
}

#+end_src

* Model
Extracted from here: https://medium.com/@oluyaled/audio-classification-using-deep-learning-and-tensorflow-a-step-by-step-guide-5327467ee9ab

#+begin_src python :results output value :tangle /Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/model.py
  import os
  import librosa
  import numpy as np
  import tensorflow as tf
  from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense
  from tensorflow.keras.models import Model
  from tensorflow.keras.optimizers import Adam
  from sklearn.model_selection import train_test_split
  from tensorflow.keras.utils import to_categorical
  from tensorflow.image import resize
  from tensorflow.keras.models import load_model

  # Define your folder structure
  cwd = os.getcwd()

  dir = '/Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/'
  # Change this to cwd + '/' + when run as script.
  data_dir = os.path.join(dir, 'training_data')
  classes = ['Impulse', 'Iteration', 'Vsustain', 'Fsustain']
  print(data_dir)
  
  # Load and preprocess audio data
  def load_and_preprocess_data(data_dir, classes, target_shape=(128, 128)):
      data = []
      labels = []
      
      for i, class_name in enumerate(classes):
          class_dir = os.path.join(data_dir, class_name)
          for filename in os.listdir(class_dir):
              if filename.endswith('.wav'):
                  file_path = os.path.join(class_dir, filename)
                  audio_data, sample_rate = librosa.load(file_path, sr=None)
                  # Perform preprocessing (e.g., convert to Mel spectrogram and resize)
                  mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)
                  mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)
                  data.append(mel_spectrogram)
                  labels.append(i)
                  
      return np.array(data), np.array(labels)

  # Split data into training and testing sets
  data, labels = load_and_preprocess_data(data_dir, classes)
  labels = to_categorical(labels, num_classes=len(classes))  # Convert labels to one-hot encoding
  X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

  # Create a neural network model
  input_shape = X_train[0].shape
  input_layer = Input(shape=input_shape)
  x = Conv2D(32, (3, 3), activation='relu')(input_layer)
  x = MaxPooling2D((2, 2))(x)
  x = Conv2D(64, (3, 3), activation='relu')(x)
  x = MaxPooling2D((2, 2))(x)
  x = Flatten()(x)
  x = Dense(64, activation='relu')(x)
  output_layer = Dense(len(classes), activation='softmax')(x)
  model = Model(input_layer, output_layer)
#+end_src

#+RESULTS:

* Current directory
#+begin_src python :results value output
    import os
    cwd = os.getcwd() + "/code.org"
    file = print(os.path.basename(cwd))
    print(cwd)
#+end_src

#+RESULTS:
: code.org
: /Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/stuff/code.org
* Stuff
#+begin_src python :results value output
  import os
  import glob
  directory = "/Users/henrik_frisk/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data"
  extension = "*.wav"

  for audio_file in glob.glob(os.path.join(directory, extension)):
          print(f"Found audio file {audio_file}")      
#+end_src

#+RESULTS:
: Found audio file /Users/henrik_frisk/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data/pluck.wav
: Found audio file /Users/henrik_frisk/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data/imp_005.wav
: Found audio file /Users/henrik_frisk/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data/rhythm.wav
: Found audio file /Users/henrik_frisk/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data/tone.wav
: Found audio file /Users/henrik_frisk/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data/fsustain-1.wav
: Found audio file /Users/henrik_frisk/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data/iter_009.wav
: Found audio file /Users/henrik_frisk/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data/noise.wav

* Compiling the model
#+begin_src python :tangle /Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/model.py
  model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
#+end_src

* Training the model
#+begin_src python :tangle /Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/model.py
  model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))
#+end_src

* Save the model
The 'dir' variable is a hack to handle emacs directories. This should be replaced by os.getcwd().
#+begin_src python :tangle /Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/save.py
  import sys
#  file_name = sys.argv[1]
#  dir = os.getcwd()
  dir = '/Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/'
  model.save(dir + 'audio_classification_imp_iter.keras')
#+end_src

* Model evaluation
#+begin_src python :tangle /Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/eval_model.py
  test_accuracy=model.evaluate(X_test,y_test,verbose=0)
  print(test_accuracy[1])
#+end_src

* Testing the model
This proves to be working with limited tests. Next thing to do is work out the optimal settings for analysis below. Especially the spectrogram settings and we should also test with other spectrograms than mel.

#+begin_src python :results value output :tangle /Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/eval_model.py
  import glob
  # Load the saved model
  # dir = os.getcwd()
  dir = '/Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/'
  model = load_model(dir + 'audio_classification_imp_iter.keras')

  # Define the target shape for input spectrograms
  target_shape = (128, 128)

  # Define your class labels
  classes = ['Impulse', 'Iteration', 'Vsustain', 'Fsustain']

  # Function to preprocess and classify an audio file
  def test_audio(file_path, model):
      # Load and preprocess the audio file
      audio_data, sample_rate = librosa.load(file_path, sr=None)
      audio_data = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)
      audio_data = resize(np.expand_dims(audio_data, axis=-1), target_shape)
      audio_data = tf.reshape(audio_data, (1,) + target_shape + (1,))
          
      # Make predictions
      predictions = model.predict(audio_data)
      
      # Get the class probabilities
      class_probabilities = predictions[0]
      
      # Get the predicted class index
      predicted_class_index = np.argmax(class_probabilities)
      
      return class_probabilities, predicted_class_index

  # Test an audio file
  test_audio_dir = '/Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data'

  extension = "*.wav"
  for test_audio_file in glob.glob(os.path.join(test_audio_dir, extension)):
      class_probabilities, predicted_class_index = test_audio(test_audio_file, model)

      # Display results for all classes
      for i, class_label in enumerate(classes):
          probability = class_probabilities[i]
          print(f'Class: {class_label}, Probability: {probability:.4f}')

          # Calculate and display the predicted class and accuracy
          predicted_class = classes[predicted_class_index]
          accuracy = class_probabilities[predicted_class_index]
          print(f'The audio {os.path.basename(test_audio_file)} is classified as: {predicted_class}')
          print(f'Accuracy: {accuracy:.4f}')
#+end_src

* Testing librosa features
#+name: plotme
#+begin_src python :session :results value :noweb yes :tangle test_audio.py :var myvar=3
  import os
  import librosa
  import numpy as np
  import matplotlib.pyplot as plt

  dir = '/Volumes/Freedom/Dropbox/Documents/kmh/forskning/applications/KK/KKS 2022 IRESAP/audio_classification/classification/training_data/'
  classes = ['Impulse', 'Iteration', 'Vsustain', 'Fsustain']
  data_dir = os.path.join(dir, classes[1])
  afile = os.path.join(data_dir, '2.wav')

  print(afile)
  y, sr = librosa.load(afile, sr=None)
#+end_src

#+RESULTS: plotme

** melspectrogram
#+begin_src python :noweb yes :tangle melspec.py 
  <<plotme>>
  
  S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)

  fig, ax = plt.subplots()
  S_dB = librosa.power_to_db(S, ref=np.max)
  img = librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000, ax=ax)
  fig.colorbar(img, ax=ax, format='%+2.0f dB')
  ax.set(title='Mel-frequency spectrogram')

  plt.show()
#+end_src

** mfcc
#+begin_src python :noweb yes :tangle mfcc.py
  <<plotme>>
  mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
  S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
  fig, ax = plt.subplots(nrows=2, sharex=True)
  img = librosa.display.specshow(librosa.power_to_db(S, ref=np.max),
                                 x_axis='time', y_axis='mel', fmax=8000,
                                 ax=ax[0])
  fig.colorbar(img, ax=[ax[0]])
  ax[0].set(title='Mel spectrogram')
  ax[0].label_outer()
  img = librosa.display.specshow(mfccs, x_axis='time', ax=ax[1])
  fig.colorbar(img, ax=[ax[1]])
  ax[1].set(title='MFCC')
  plt.show()
#+end_src

** beat detections
#+begin_src python :noweb yes :tangle beat.py
  <<plotme>>
  import scipy.stats
  onset_env = librosa.onset.onset_strength(y=y, sr=sr)
  pulse = librosa.beat.plp(onset_envelope=onset_env, sr=sr)
  # Or compute pulse with an alternate prior, like log-normal

  prior = scipy.stats.lognorm(loc=np.log(120), scale=120, s=1)
  pulse_lognorm = librosa.beat.plp(onset_envelope=onset_env, sr=sr,
                                   prior=prior)
  melspec = librosa.feature.melspectrogram(y=y, sr=sr)
  fig, ax = plt.subplots(nrows=3, sharex=True)
  librosa.display.specshow(librosa.power_to_db(melspec,
                                               ref=np.max),
                           x_axis='time', y_axis='mel', ax=ax[0])

  ax[0].set(title='Mel spectrogram')
  ax[0].label_outer()
  ax[1].plot(librosa.times_like(onset_env),
             librosa.util.normalize(onset_env),
             label='Onset strength')
  ax[1].plot(librosa.times_like(pulse),
              librosa.util.normalize(pulse),
               label='Predominant local pulse (PLP)')
  ax[1].set(title='Uniform tempo prior [30, 300]')
  ax[1].label_outer()
  ax[2].plot(librosa.times_like(onset_env),
               librosa.util.normalize(onset_env),
               label='Onset strength')
  ax[2].plot(librosa.times_like(pulse_lognorm),
               librosa.util.normalize(pulse_lognorm),
               label='Predominant local pulse (PLP)')
  ax[2].set(title='Log-normal tempo prior, mean=120', xlim=[5, 20])
  ax[2].legend()

  plt.show()
#+end_src

* Test result
A first quick run just testing two files, one in each category, was successful:

>>> 
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step
>>> 
Class: Impulse, Probability: 0.9994
Class: Iteration, Probability: 0.0006
>>> 
The audio is classified as: Impulse
Accuracy: 0.9994
>>> 
>>> 
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step
>>> 
Class: Impulse, Probability: 0.1112
Class: Iteration, Probability: 0.8888
The audio is classified as: Iteration
Accuracy: 0.8888
>>> 
